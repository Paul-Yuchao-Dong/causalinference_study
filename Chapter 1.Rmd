---
title: "Chapter 1"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(dagitty)

```
## Study Question 1.4.1
```{r}
g <- dagitty('dag {
    X [pos="0,1"]
    Y [pos="1,1"]
    Z [pos="2,1"]
    W [pos="1,0"]
    T [pos="2,2"]

  X->W->Z->T
  X->Y->Z->T
  W->Y
  Y->T
}')
plot(g)
```
```{r variance prop}
N <- 100000
X <- rnorm(N)
Y <- X/3 + rnorm(N)
Z <- Y/16 + rnorm(N)
```

```{r}
var(X)
```
```{r}
var(Y)
```

```{r}
summary(lm(X~Y) )
```
```{r}
fig2.5 <- dagitty("dag {
    X -> R -> S -> T <- U <- V -> Y
}")
plot( graphLayout( fig2.5 ) )
```
```{r}
paths(fig2.5, "X", "Y", c("R", "V"))
```
```{r}
library(purrr)
```

```{r}
pairs <- combn(c("X","S","T","U","Y"), 2, simplify = F)
walk(pairs,  function(x) {
  p <- paths(fig2.5, x[1], x[2], c("R", "V"))
  if (!p$open) message( x[1], " and ", x[2], " are conditionally independent given {R,V}")
  else message( x[1], " and ", x[2], " are possibly dependent given {R,V}")
})

```
```{r}
fig2.6 <- dagitty("dag {
                  X->R->S->T<-U<-V->Y
                  T->P
}")
variables <- unlist(strsplit("X S T U V Y", split = " ") )
```
```{r}
pairs <- combn(variables, 2, simplify = F)
walk(pairs,  function(x) {
    if (dseparated(fig2.6, x[1], x[2], c("R", "P"))) message(x[1], " and ", x[2], " are conditionally independent given {R, P}")
})
```
```{r}
impliedConditionalIndependencies(fig2.6)
```
```{r}
map_lgl(set_names(names(fig2.5)), function(Z) {
  dseparated(fig2.5, "Y", "X", Z)
})
```
```{r}
d <- simulateSEM(fig2.5, .7,.7, N=1e4)
lm(Y ~ X + V, data = d) %>% confint
```
```{r}
lm(Y ~ X + T, data = d) %>% confint

```

```{r}
d <- simulateSEM(fig2.6, .7, .7, N = 1e4)
lm(Y ~ X + R + S + T + P, data = d) %>% confint
```

```{r}
fig2.9 <- dagitty("dag { 
X <- Z1 -> Z3 <- Z2 -> Y
X <- Z3 -> Y
X -> W -> Y
}")
coordinates(fig2.9) <- list(x=c(X=1,W=2,Y=3,Z1=1,Z3=2,Z2=3),
    y=c(X=0,W=0,Y=0,Z2=-2,Z1=-2,Z3=-1))
plot(fig2.9)
```
```{r}
dseparated(fig2.9, "X","Y", c("W", "Z3"))
```
```{r}
paths(fig2.9, "X","Y", c("W", "Z3"))
```
```{r}
paths(fig2.9, "Z1", "W", "X")
```
```{r}
paths(fig2.9, "Z3", "W", "X")
```
```{r}
latents(fig2.9)<-  setdiff(names(fig2.9) , c("Z3", "W", "X", "Z1"))
impliedConditionalIndependencies(fig2.9)
```
```{r}
pairs <- combn(names(fig2.9), 2 , simplify = F)
walk(pairs, function(x){
  all_others <- setdiff(names(fig2.9), x)
  if (dseparated(fig2.9, x[1], x[2], all_others)) 
    message(x[1], " and ", x[2], " are conditionally independent given ", 
            paste(all_others, collapse = " ,"))
})
```
```{r}
paths(fig2.9, "Z3", "Z2")
```

## Suppose we wish to predict the value of Z2 from measurements of Z3. Would the quality of our prediction improve if we add measurement of W? Explain.

This is a tough question. Statistically speaking, it is well known that a (within-sample) prediction can never get worse by adding more predictors. However, we have learnt before that it does not help to add predictors that are d-separated from the outcome variable by the other predictors (in other words, once we include all variables in the Markov blanket prediction, we are good). So the first thing to check is whether W is d-separated from Z2 given Z3. This is not the case, as can be easily seen by path inspection:

```{r}
p <- paths(fig2.9, "W", "Z2", "Z3")
p
```

```{r}
latents(fig2.9) <- NULL
d <- simulateSEM(fig2.9, .4, .4, N = 1e4)
lm(Z2 ~ Z3, data = d) %>% summary()
lm(Z2 ~ Z3 + W, data = d) %>% summary()

```

